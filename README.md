# POC â€“ Pipeline de gestion de tickets clients en temps rÃ©el

## Etape 1

## 1. Objectif

Ce projet consiste Ã  simuler un flux de tickets clients en temps rÃ©el en utilisant **Redpanda** (Kafka-compatible) et **Python**.  
Lâ€™objectif est de produire et consommer des donnÃ©es de tickets pour prÃ©parer lâ€™analyse en temps rÃ©el avec **PySpark** dans les Ã©tapes suivantes.
Dans le cadre de la migration de lâ€™infrastructure dâ€™InduTechData vers AWS et Redpanda, ce projet a pour objectif de rÃ©aliser un Proof Of Concept (POC) dâ€™un systÃ¨me de gestion de tickets clients en temps rÃ©el.

## 2. PrÃ©requis

- Docker & Docker Compose  
- Python 3.8+  
- pip ou conda pour installer les packages Python nÃ©cessaires  

## 3. Lancement de Redpanda avec Docker Compose

CrÃ©ez un fichier `docker-compose.yml` avec le contenu suivant :  

> Voir le fichier `docker-compose.yml` (code provenant du site de Redpanda)

Lancer Redpanda et la console :  

```bash
docker-compose up -d
```


VÃ©rifier que Redpanda fonctionne :
```bash
docker ps
```

Le broker Redpanda Ã©coute sur localhost:19092

La console Redpanda accessible sur http://localhost:8080


## 4. CrÃ©ation dâ€™un topic

CrÃ©er un topic `client_tickets` pour stocker les tickets :
```bash
docker exec -it redpanda-0 rpk topic create client_tickets
```
Ou directement sur l'interface sur http://localhost:8080


## 5. Script Python pour produire des tickets

Exemple de script `produce.py` contenue dans le dossier `producer`


Installer les dÃ©pendances :
```bash
pip install kafka-python
```

Lancer le script :
```bash
python produce_tickets.py
```

## 6. Validation

Consommer quelques messages pour vÃ©rifier que tout fonctionne :
```bash
docker exec -it redpanda-0 rpk topic consume client_tickets -b localhost:9092 --offset earliest --limit 5
```
Ou vÃ©rifier directement dans topic que messages se sont bien importÃ©s

## 7. Conseils

- Docker Compose facilite la gestion du broker et de la console Redpanda, et Ã©vite les problÃ¨mes de configuration sur Windows.

- Tous les scripts Python peuvent Ãªtre lancÃ©s directement depuis lâ€™hÃ´te (Windows/Linux/Mac) vers le broker Redpanda exposÃ© sur localhost:19092.

## Etape 2

## Architecture mise en place (Ã‰tapes 1 & 2)
ğŸ”¹ Composants

Redpanda

- Ingestion des tickets clients en temps rÃ©el

- Topic Kafka : client_tickets (3 partitions)

Producteur Python

- GÃ©nÃ©ration de tickets alÃ©atoires

- Envoi des messages au format JSON vers Redpanda

Spark Structured Streaming

- Lecture des messages Kafka

- Transformation, enrichissement et agrÃ©gation

- Affichage des rÃ©sultats par micro-batch

Redpanda Console

- Visualisation des topics et des messages

## Flux de donnÃ©es

1 - Le script Python gÃ©nÃ¨re 200 tickets clients

2 - Les tickets sont envoyÃ©s dans le topic client_tickets

3 - Spark lit les messages depuis Redpanda

4 - Les donnÃ©es JSON sont parsÃ©es en DataFrame structurÃ©

5 - Les tickets sont enrichis avec une Ã©quipe de support

6 - Une agrÃ©gation calcule le nombre de tickets par type

7 - Les rÃ©sultats sont affichÃ©s par micro-batch dans la console Spark

## Structure du projet
```kotlin
project/
â”‚
â”œâ”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ producer.py
â”‚
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_streaming.py
â”‚
â”œâ”€â”€ ivy/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ output/
â”‚
â””â”€â”€ README.md
```
## Dossiers techniques importants
- ğŸ”¹ `ivy/`

Ce dossier est utilisÃ© par Apache Ivy, le gestionnaire de dÃ©pendances de Spark.

ğŸ‘‰ Il a Ã©tÃ© crÃ©Ã© manuellement afin de :

permettre Ã  Spark de tÃ©lÃ©charger les dÃ©pendances Kafka

Ã©viter les erreurs du type /nonexistent/.ivy2/cache

Ce dossier garantit le bon fonctionnement de lâ€™option :

```bash
--packages org.apache.spark:spark-sql-kafka-0-10_2.12
```

- ğŸ”¹ `data/checkpoints/`

Ce dossier est utilisÃ© par Spark Structured Streaming pour le checkpointing.

Le checkpoint permet :

de mÃ©moriser les offsets Kafka dÃ©jÃ  consommÃ©s

dâ€™assurer la reprise aprÃ¨s arrÃªt

dâ€™Ã©viter de relire plusieurs fois les mÃªmes messages

ğŸ‘‰ Il est obligatoire pour un pipeline streaming fiable.


- ğŸ”¹ `data/output/`

Ce dossier est destinÃ© Ã  recevoir les rÃ©sultats des analyses (Parquet, JSON, etc.).

ğŸ“Œ Ã€ ce stade du projet, il est normal quâ€™il soit vide, car :

les rÃ©sultats sont actuellement affichÃ©s uniquement dans la console (format("console"))

lâ€™export vers un fichier sera rÃ©alisÃ© Ã  lâ€™Ã©tape 3

## Traitements rÃ©alisÃ©s avec Spark

- ğŸ”¹ Parsing JSON

Les messages Kafka (bytes) sont convertis en colonnes structurÃ©es via un schÃ©ma explicite.

- ğŸ”¹ Enrichissement

Ajout automatique dâ€™une colonne support_team :

TECHNICAL â†’ Tech Support

BILLING â†’ Billing Team

ACCOUNT â†’ Account Management

GENERAL â†’ General Support

- ğŸ”¹ AgrÃ©gation

Calcul du nombre de tickets par type de demande, mis Ã  jour en continu.

## Gestion des micro-batchs

Les messages sont traitÃ©s en micro-batchs

Un checkpoint est utilisÃ© pour garantir lâ€™Ã©tat du streaming

Le rÃ©sultat final confirme la consommation des 200 tickets produits

## Lancement du projet

- PrÃ©cision, le `docker-compose.yml` a Ã©tÃ© modifiÃ© pour intÃ©grer Spark

```bash
docker-compose up
```

Redpanda Console est accessible Ã  lâ€™adresse :
```bash
http://localhost:8080
```


## Etape 3

## Architecture gÃ©nÃ©rale

- Redpanda : broker Kafka (ingestion temps rÃ©el)

- Kafka Producer (Python) : gÃ©nÃ©ration de tickets clients

- Spark Structured Streaming (Docker) :

 - lecture des messages Kafka

 - enrichissement

 - agrÃ©gation

 - export JSON final

- Docker Compose : orchestration de lâ€™ensemble


## ğŸ“ Structure du projet
```kotlin
Projet_9_Exercice2/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ producer.py
â”‚
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ spark_streaming.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â””â”€â”€ client_tickets/
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ client_tickets/
â”‚
â”œâ”€â”€ ivy/
â”‚
â””â”€â”€ README.md
```


### ğŸ“Œ Dossiers crÃ©Ã©s manuellement

- ivy/
â†’ utilisÃ© par Spark pour stocker les dÃ©pendances Maven (Kafka connector)

- data/

 - checkpoints/ : nÃ©cessaire au fonctionnement de Spark Streaming

 - output/ : stockage des rÃ©sultats

     - client_tickets/ : fichier JSON final



## ğŸ”„ Ã‰tapes 2 & 3 â€“ Traitement + Export (combinÃ©es)

#### ğŸ‘‰ Les Ã©tapes 2 et 3 ont Ã©tÃ© regroupÃ©es volontairement dans un seul pipeline Spark, afin de :

- traiter les donnÃ©es en streaming

- exporter directement le rÃ©sultat final sans script supplÃ©mentaire

### Pourquoi cette approche ?

Spark Structured Streaming ne permet pas lâ€™Ã©criture directe en JSON avec outputMode("complete")

La solution recommandÃ©e est lâ€™utilisation de foreachBatch

Cela permet dâ€™avoir :

- du streaming

- un DataFrame classique par batch

- un export final maÃ®trisÃ©

## ğŸ§  Traitement Spark (spark_streaming.py)
### Fonctions rÃ©alisÃ©es :

1. Lecture du topic Kafka client_tickets

2. Parsing JSON

3. Enrichissement :

- attribution dâ€™une Ã©quipe support selon le type

4. AgrÃ©gation :

- nombre de tickets par type

5. Export automatique des rÃ©sultats finaux en JSON

### Points techniques clÃ©s :

`startingOffsets` = earliest â†’ reprise des 200 messages de lâ€™Ã©tape 1

`maxOffsetsPerTrigger` = 50 â†’ micro-batchs contrÃ´lÃ©s

`foreachBatch` â†’ export JSON final

`checkpointLocation` â†’ reprise fiable du streaming

## ğŸ“¦ Export des rÃ©sultats (Ã‰tape 3)

Le fichier final est gÃ©nÃ©rÃ© automatiquement ici :
```bash
data/output/client_tickets/
â””â”€â”€ part-00000-xxxx.json
```

Contenu final (exemple) :
```bash
{"type":"ACCOUNT","ticket_count":52}
{"type":"BILLING","ticket_count":50}
{"type":"GENERAL","ticket_count":53}
{"type":"TECHNICAL","ticket_count":45}
```

âœ” Total = 200 tickets
âœ” Tous les messages ont Ã©tÃ© traitÃ©s
âœ” Aucune perte de donnÃ©es

## ğŸ³ Lancement du pipeline complet

Une fois les scripts prÃªts :
```bash
docker-compose up -d
```

Ce lancement :

- dÃ©marre Redpanda

- dÃ©marre Spark

- relit les 200 messages existants

- traite et exporte automatiquement les rÃ©sultats

## âœ… RÃ©sultat final

âœ” Pipeline temps rÃ©el fonctionnel

âœ” Traitement Spark validÃ©

âœ” Export JSON conforme Ã  lâ€™Ã©tape 3

âœ” Ã‰tapes 2 et 3 correctement combinÃ©es

âœ” Projet entiÃ¨rement reproductible avec Docker

ğŸ Conclusion

Ces Ã©tapes du projet dÃ©montre la mise en Å“uvre complÃ¨te dâ€™un pipeline temps rÃ©el industriel, depuis lâ€™ingestion Kafka jusquâ€™Ã  lâ€™export de donnÃ©es analysÃ©es, en sâ€™appuyant sur des outils standards du Data Engineering moderne.